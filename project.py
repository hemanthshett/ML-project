# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lyFaie6TdxUCrqAbA5UqaLJkuIEiu6Li
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns

df=pd.read_csv('/content/drive/MyDrive/heart.csv')

df.head()

df.shape

df.info()

df.isnull().values.any()

x=df.drop('target',axis=1)
y=df['target']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)
x_scaled

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaled=scaler.fit_transform(x)
scaled

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.20,random_state=1)

"""Classification Algorithms

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(x_train,y_train)
y_pred_logreg = logreg.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_test,y_pred_logreg))
print('Accuracy',accuracy_score(y_test,y_pred_logreg))
print('Precision',precision_score(y_test,y_pred_logreg))
print('Recall',recall_score(y_test,y_pred_logreg))
print('f1 score',f1_score(y_test,y_pred_logreg))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)                       #Number of neighbors = K
knn.fit(x_train,y_train)
y_pred_knn=knn.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_test,y_pred_knn))
print('Accuracy',accuracy_score(y_test,y_pred_knn))
print('Precision',precision_score(y_test,y_pred_knn))
print('Recall',recall_score(y_test,y_pred_knn))
print('f1 score',f1_score(y_test,y_pred_knn))

"""Decision Tree"""

#Criteria - Gini Index
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(x_train,y_train)
y_pred_dt=dtree.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_test,y_pred_dt))
print('Accuracy',accuracy_score(y_test,y_pred_dt))
print('Precision',precision_score(y_test,y_pred_dt))
print('Recall',recall_score(y_test,y_pred_dt))
print('f1 score',f1_score(y_test,y_pred_dt))

#Criteria - Entropy
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier(criterion='entropy')
dtree.fit(x_train,y_train)
y_pred_dt=dtree.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_test,y_pred_dt))
print('Accuracy',accuracy_score(y_test,y_pred_dt))
print('Precision',precision_score(y_test,y_pred_dt))
print('Recall',recall_score(y_test,y_pred_dt))
print('f1 score',f1_score(y_test,y_pred_dt))

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
fn = list(x_train)
cn = ['0','1']
plt.figure(figsize=(4,4), dpi=300)
plot_tree(dtree, feature_names=fn, class_names=cn, filled=True);

"""SVM"""

from sklearn import svm
clf = svm.SVC(kernel='linear',C=0.01)
clf.fit(x_train,y_train)
y_pred_svm = clf.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, f1_score, recall_score
print(confusion_matrix(y_test,y_pred_svm))
print('Accuracy',accuracy_score(y_test,y_pred_svm))
print('classification_report',classification_report(y_test, y_pred_svm))
print('Precision',precision_score(y_test,y_pred_svm))
print('f1',f1_score(y_test,y_pred_svm))
print('recall',recall_score(y_test,y_pred_svm))

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier( n_estimators=10)
rf_classifier.fit(x_train,y_train)
y_pred_rf = rf_classifier.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_rf))
print('Accuracy',accuracy_score(y_test,y_pred_rf))
print('classification_report',classification_report(y_test, y_pred_rf))
print('Precision',precision_score(y_test,y_pred_rf))
print('f1',f1_score(y_test,y_pred_rf))
print('recall',recall_score(y_test,y_pred_rf))

#Gradient Booster
from sklearn.ensemble import GradientBoostingClassifier
gradient_booster = GradientBoostingClassifier( learning_rate=0.1)
gradient_booster.fit(x_train,y_train)
y_pred_gradboost = gradient_booster.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_gradboost))
print('Accuracy',accuracy_score(y_test,y_pred_gradboost))
print('classification_report',classification_report(y_test, y_pred_gradboost))
print('Precision',precision_score(y_test,y_pred_gradboost))
print('f1',f1_score(y_test,y_pred_gradboost))
print('recall',recall_score(y_test,y_pred_gradboost))

#Adaboost
from sklearn.ensemble import AdaBoostClassifier
adaboost_classifier = AdaBoostClassifier()
adaboost_classifier.fit(x_train,y_train)
y_pred_adab = adaboost_classifier.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_adab))
print('Accuracy',accuracy_score(y_test,y_pred_adab))
print('classification_report',classification_report(y_test, y_pred_adab))
print('Precision',precision_score(y_test,y_pred_adab))
print('f1',f1_score(y_test,y_pred_adab))
print('recall',recall_score(y_test,y_pred_adab))

pip install xgboost

#xgboost
from xgboost import XGBClassifier
model = XGBClassifier( learning_rate=1)
model.fit(x_train,y_train)
y_pred_xgb = model.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_xgb))
print('Accuracy',accuracy_score(y_test,y_pred_xgb))
print('classification_report',classification_report(y_test, y_pred_xgb))
print('Precision',precision_score(y_test,y_pred_xgb))
print('f1',f1_score(y_test,y_pred_xgb))
print('recall',recall_score(y_test,y_pred_xgb))

pip install catboost

from catboost import CatBoostClassifier
model1 = CatBoostClassifier()
model1.fit(x_train,y_train)
y_pred_cat = model1.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_cat))
print('Accuracy',accuracy_score(y_test,y_pred_cat))
print('classification_report',classification_report(y_test, y_pred_cat))
print('Precision',precision_score(y_test,y_pred_cat))
print('f1',f1_score(y_test,y_pred_cat))
print('recall',recall_score(y_test,y_pred_cat))

"""Ensemble model"""

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

le = LabelEncoder()

all_labels = np.concatenate((y_pred_logreg, y_pred_svm, y_pred_dt, y_pred_rf, y_test))
le.fit(all_labels)

X = np.array([y_pred_logreg,y_pred_svm, y_pred_dt,y_pred_rf]).T

meta_learner = RandomForestClassifier()
meta_learner.fit(X, y_test)
ensemble_predictions = meta_learner.predict(X)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,ensemble_predictions))
print(accuracy_score(y_test,ensemble_predictions))
print(classification_report(y_test,ensemble_predictions))

!pip install lime
import lime
import lime.lime_tabular

from sklearn import svm
clf = svm.SVC(kernel = 'linear', C=0.01)
clf.fit(x_train, y_train)
y_pred_svm= clf.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(y_test,y_pred_svm))
print('Accuracy',accuracy_score(y_test,y_pred_svm))
print('classification_report',classification_report(y_test, y_pred_svm))

from lime.lime_tabular import LimeTabularExplainer
explainer = LimeTabularExplainer(x_train.values, feature_names=x_train.columns.values.tolist(), class_names=['TYPE'], mode='regression')
exp = explainer.explain_instance(x_train.values[100], clf.predict, num_features=5)
exp.as_pyplot_figure()
from matplotlib import pyplot as plt
plt.tight_layout()

exp.show_in_notebook(show_table=True)

pip install shapash

model=RandomForestClassifier(max_depth=5, random_state=42, n_estimators=12)
model2=model.fit(x_train, y_train)
rf_y_pred = model2.predict(x_test)
rf_y_pred

fi=pd.DataFrame({'Feature': x_train.columns, 'Importance':model2.feature_importances_})
fi.sort_values(by='Importance',ascending=False, ignore_index=True)

from shapash.explainer.smart_explainer import SmartExplainer
xpl= SmartExplainer(model2)
xpl.compile(x=x_test)
xpl.plot.features_importance()
#Global explaination

import random
subset = random.choices(x_test.index, k=50)
xpl.plot.features_importance(selection=subset)

xpl.plot.local_plot(index=random.choice(x_test.index))

"""Manual Hyperparameter tuning"""

model=RandomForestClassifier(n_estimators=100, criterion='entropy', max_features='sqrt', min_samples_leaf=20, random_state=100)

model.fit(x_train,y_train)
predictions=model.predict(x_test)
print(confusion_matrix(y_test,predictions))
print(accuracy_score(y_test,predictions))
print(classification_report(y_test, predictions))

import numpy as np
from sklearn.model_selection import RandomizedSearchCV


n_estimators= [int(x) for x in np.linspace(start = 200, stop = 2000, num=10)]
max_features = ['auto','sqrt','log2']
max_depth=[int(x) for x in np.linspace(10,1000,10)]
min_samples_split= [2,5,10,14]
min_samples_leaf=[1,2,4,6,8]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'criterion':['entropy','gini']}
print(random_grid)

rf=RandomForestClassifier()
rf_randomcv=RandomizedSearchCV( estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2,
                               random_state=100, n_jobs=-1)
rf_randomcv.fit(x_train,y_train)

rf_randomcv.best_params_

best_random_grid =rf_randomcv.best_estimator_
from sklearn.metrics import accuracy_score
y_pred = best_random_grid.predict(x_test)
print('Accuracy score {}'.format(accuracy_score(y_test,y_pred)))
print("Classification report: {}".format(classification_report(y_test,y_pred)))

"""Grid Search CV"""

rf_randomcv.best_params_

from sklearn.model_selection import GridSearchCV

param_grid = {
    'criterion': [rf_randomcv.best_params_['criterion']],
    'max_depth': [rf_randomcv.best_params_['max_depth']],
    'max_features': [rf_randomcv.best_params_['max_features']],
    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'],
                         rf_randomcv.best_params_['min_samples_leaf']+2,
                         rf_randomcv.best_params_['min_samples_leaf'] + 4],
    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,
                          rf_randomcv.best_params_['min_samples_split'] - 1,
                          rf_randomcv.best_params_['min_samples_split'],
                          rf_randomcv.best_params_['min_samples_split'] +1,
                          rf_randomcv.best_params_['min_samples_split'] + 2],
    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100,
                     rf_randomcv.best_params_['n_estimators'],
                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]
}

print(param_grid)

rf=RandomForestClassifier()
grid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)
grid_search.fit(x_train,y_train)

grid_search.best_estimator_

best_grid=grid_search.best_estimator_

best_grid

y_pred=best_grid.predict(x_test)
print(confusion_matrix(y_test,y_pred))
print((accuracy_score(y_test,y_pred)))
#print("Classification report: {}".format(classification_report(y_test,y_pred)))

